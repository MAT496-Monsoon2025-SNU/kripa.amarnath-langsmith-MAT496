{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f680fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy retriever invoked with: Can Metadata be added during runtime?\n",
      "<think>\n",
      "Okay, the user is asking if metadata can be added during runtime. I remember from Document 1 that it's possible but might need specific libraries or frameworks. Then, Document 2 mentioned that some systems support this without downtime. So, I should combine both points, mention the possibility, and note that it depends on the setup. I'll keep it concise, three sentences max.\n",
      "</think>\n",
      "\n",
      "Yes, metadata can typically be added during runtime depending on the system or framework being used. Some systems allow dynamic modification of metadata without requiring downtime or recompilation. However, the specific capabilities depend on the tools and technologies in place.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Can Metadata be added during runtime?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"runtime_metadata\": \"foo\"}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd694a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy retriever invoked with: How can Metadata be incorporated in @traceable?\n",
      "<think>\n",
      "Okay, the user is asking how metadata can be incorporated into @traceable. They've mentioned that I should use the provided context to answer.\n",
      "\n",
      "First, I'll look at the context from Document 1. It talks about tagging data points and embedding info for tracking. That seems like adding metadata at the source or during processing would help with traceability and organization.\n",
      "\n",
      "Next, Document 2 mentions standardized templates and centralized registries. Creating templates would ensure consistency, and having a registry would make the metadata easily accessible for everyone involved.\n",
      "\n",
      "I should combine these points into a concise answer. Start with tagging and embedding, then mention templates and registries for consistency and accessibility. That should cover the main methods.\n",
      "</think>\n",
      "\n",
      "Metadata can be incorporated into @traceable by tagging data points with relevant information and embedding metadata during data processing. Additionally, standardized metadata templates and centralized registries can ensure consistency and accessibility across the system. This approach enhances traceability and organization of data within @traceable.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"How can Metadata be incorporated in @traceable?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a61f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tempfile\n",
    "import nest_asyncio\n",
    "from groq import Groq\n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "from langsmith import traceable\n",
    "from typing import List\n",
    "\n",
    "\n",
    "MODEL_PROVIDER = \"groq\" \n",
    "MODEL_NAME = \"deepseek-r1-distill-llama-70b\" \n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_vector_db_retriever(api_key: str): \n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "    embed = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "    class DummyRetriever:\n",
    "        def invoke(self, question):\n",
    "            print(f\"Dummy retriever invoked with: {question}\")\n",
    "            return [\n",
    "                type('Document', (object,), {'page_content': f\"Relevant info for '{question}': Document 1.\"})(),\n",
    "                type('Document', (object,), {'page_content': f\"Relevant info for '{question}': Document 2.\"})()\n",
    "            ]\n",
    "    return DummyRetriever()\n",
    "\n",
    "\n",
    "GROQ_API_KEY = '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'  \n",
    "OPENAI_EMBEDDING_API_KEY = \"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\" # Your OpenAI key for embeddings\n",
    "\n",
    "groq_client = Groq(api_key=GROQ_API_KEY) \n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever(api_key=OPENAI_EMBEDDING_API_KEY) \n",
    "\n",
    "\n",
    "\n",
    "@traceable(\n",
    "    metadata={\"vectordb\": \"sklearn\", \"embedding_model\": \"openai_text-embedding-ada-002\"}\n",
    ")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str, documents: List[dict]):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_groq(messages) \n",
    "\n",
    "@traceable(\n",
    "    metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER}\n",
    ")\n",
    "def call_groq(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    \n",
    "    return groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable\n",
    "def langsmith_rag(question: str, langsmith_extra=None): \n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8540b050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy retriever invoked with: What are the features of @traceable in langsmith?\n",
      "Generating response for: What are the features of @traceable in langsmith? with docs: [<class '__main__.Document'>, <class '__main__.Document'>]\n",
      "AI answer for What are the features of @traceable in langsmith?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What are the features of @traceable in langsmith?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a035b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question, retriever)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f5b4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groq_client = Groq(api_key=GROQ_API_KEY) \n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever(api_key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02c969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GROQ_API_KEY = \"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\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189f3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tempfile\n",
    "import nest_asyncio\n",
    "from groq import Groq \n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "\n",
    "\n",
    "def get_vector_db_retriever(api_key: str): \n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "    embed = OpenAIEmbeddings(api_key=api_key) \n",
    "   \n",
    "    class DummyRetriever:\n",
    "        def invoke(self, question):\n",
    "            print(f\"Dummy retriever invoked with: {question}\")\n",
    "            \n",
    "            return [\n",
    "                type('Document', (object,), {'page_content': \"dummy doc 1\"}),\n",
    "                type('Document', (object,), {'page_content': \"dummy doc 2\"})\n",
    "            ]\n",
    "    return DummyRetriever()\n",
    "\n",
    "\n",
    "def retrieve_documents(question: str, retriever_instance):\n",
    "    return retriever_instance.invoke(question)\n",
    "\n",
    "\n",
    "class DummyMessage:\n",
    "    def __init__(self, content): self.content = content\n",
    "class DummyChoice:\n",
    "    def __init__(self, content): self.message = DummyMessage(content)\n",
    "class DummyResponse:\n",
    "    def __init__(self, content): self.choices = [DummyChoice(content)]\n",
    "def generate_response(question: str, documents):\n",
    "    print(f\"Generating response for: {question} with docs: {documents}\")\n",
    "    return DummyResponse(f\"AI answer for {question}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "179616bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAT496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
