{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ac1a3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out the main benefit of using open-source embedding models. Let me start by recalling what I know about embeddings and open-source models. \n",
      "\n",
      "Embeddings are a way to represent text (or other data) as vectors in a high-dimensional space, which can then be used for various NLP tasks like semantic search, clustering, or classification. Open-source means that the models are freely available for anyone to use, modify, and distribute. \n",
      "\n",
      "The context provided mentions Hugging Face as a provider of open-source models for embeddings. It also notes that these models can be run locally or via an API. So, one benefit could be cost-effectiveness since open-source models are free. \n",
      "\n",
      "Another point is customization. Since the models are open-source, users can tweak them to fit specific needs, which might not be possible with proprietary models. This could be especially useful for domain-specific applications where a general model might not perform well. \n",
      "\n",
      "Additionally, using open-source models might reduce dependency on third-party services, giving more control over data and deployment. This can be crucial for organizations with strict data privacy requirements or those who want to avoid vendor lock-in. \n",
      "\n",
      "On the other hand, open-source models might require more expertise to implement and maintain, especially for those without prior experience. But the main benefits likely revolve around accessibility, cost, and flexibility. \n",
      "\n",
      "So, putting it together, the main benefit is probably accessibility and flexibility, allowing developers to use and customize the models without licensing restrictions or costs.\n",
      "</think>\n",
      "\n",
      "The main benefit of using open-source embedding models is their accessibility and flexibility, enabling developers to use and customize them without licensing restrictions or costs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What is the main benefit of using open-source embedding models?\" \n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad501abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS vector database from faiss_index_hf with Hugging Face embeddings...\n",
      "FAISS vector database loaded.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import RunTree\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils_hf_embeddings import get_hf_vector_db_retriever \n",
    "import os \n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "groq_client = Groq()r\n",
    "retriever = get_hf_vector_db_retriever() \n",
    "\n",
    "def retrieve_documents(parent_run: RunTree, question: str):\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Retrieve Documents\",\n",
    "        run_type=\"retriever\",\n",
    "        inputs={\"question\": question},\n",
    "    )\n",
    "    documents = retriever.invoke(question)\n",
    "    child_run.end(outputs={\"documents\": documents})\n",
    "    child_run.post()\n",
    "    return documents\n",
    "\n",
    "def generate_response(parent_run: RunTree, question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Generate Response\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question, \"documents\": documents},\n",
    "    )\n",
    "    messages = [ \n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    groq_response = call_groq(child_run, messages)\n",
    "    child_run.end(outputs={\"groq_response\": groq_response})\n",
    "    child_run.post()\n",
    "    return groq_response\n",
    "\n",
    "def call_groq(\n",
    "    parent_run: RunTree, messages: List[dict], model: str = \"deepseek-r1-distill-llama-70b\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Groq Call\",\n",
    "        run_type=\"llm\",\n",
    "        inputs={\"messages\": messages},\n",
    "    )\n",
    "    groq_response = groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    child_run.end(outputs={\"groq_response\": groq_response})\n",
    "    child_run.post()\n",
    "    return groq_response\n",
    "\n",
    "def langsmith_rag(question: str):\n",
    "    root_run_tree = RunTree(\n",
    "        name=\"Chat Pipeline\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question}\n",
    "    )\n",
    "\n",
    "    documents = retrieve_documents(root_run_tree, question)\n",
    "    response = generate_response(root_run_tree, question, documents)\n",
    "    output = response.choices[0].message.content\n",
    "\n",
    "    root_run_tree.end(outputs={\"generation\": output})\n",
    "    root_run_tree.post()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "667175c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "\n",
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1af7f73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a259b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how many legs a cat has. Hmm, I'm pretty sure cats are animals, and most animals have legs, but I'm not exactly sure about the exact number. Let me think about what I know about cats.\n",
      "\n",
      "First, I remember seeing cats in pictures and in real life. They have this typical posture, usually sitting or walking on the ground. I think they have four legs because when I look at a cat, I can see two in the front and two in the back. Wait, but sometimes when a cat is sitting, it's a bit hard to see all the legs clearly because they might be tucked under the body.\n",
      "\n",
      "Let me try to visualize a cat. So, from the front, a cat has two legs on the left and two on the right, making four in total. But I'm not entirely certain. Maybe I should think about other animals. Dogs, for example, have four legs too. So, maybe cats are similar. But wait, are there any animals that have a different number of legs? Like, I know spiders have eight legs, but that's different because they're arachnids, not mammals.\n",
      "\n",
      "Cats are mammals, right? And most mammals have four legs. Horses, cows, dogsâ€”all have four legs. So, that makes me think cats do too. But I'm still a bit unsure because sometimes when I see a cat sitting, it looks like it's using its hind legs more prominently, and the front legs might be hidden. Maybe I'm confusing the legs with the paws? No, paws are the feet, so each leg ends with a paw.\n",
      "\n",
      "Wait, maybe I should count them. If I imagine a cat standing up, it has two legs in the front, each ending with a paw, and two legs in the back, each also ending with a paw. That makes four legs in total. Yeah, that makes sense. Plus, I've heard people refer to cats as having four legs before, so that reinforces the idea.\n",
      "\n",
      "I guess another way to think about it is by looking at the skeleton of a cat. I don't have one in front of me, but I know that quadruped mammals have four legs. Since cats are quadrupeds, they should have four legs. Each leg is connected to the spine, with two in the front and two in the back. So, that adds up to four.\n",
      "\n",
      "Wait, but sometimes cats can lose a leg due to an accident, but that's not common. Normally, they have four. So, I think I'm confident now that the answer is four legs.\n",
      "</think>\n",
      "\n",
      "A cat has four legs. This conclusion is based on the typical anatomy of quadruped mammals, which includes cats, and the observation of their posture and movement. Each leg is connected to the spine, with two in the front and two in the back, making a total of four legs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langsmith import trace \n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many legs does a cat have?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "with trace(\"Groq Direct Call\", run_type=\"llm\", extra={\"metadata\": {\"foo\": \"bar\"}}) as t:\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "    \n",
    "    t.end(outputs={\"response\": response})\n",
    "    print(response.choices[0].message.content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d3d3784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how Hugging Face contributes to NLP. Let me start by recalling what I know about Hugging Face. From the context provided, it mentions that Hugging Face offers open-source models for various NLP tasks, including embeddings. These models can be run locally or via their inference API. \n",
      "\n",
      "Hmm, so Hugging Face provides models, which is a big part of NLP. But what else? The context also talks about LangChain, which is a framework for building applications with language models. It enables context-aware, reasoning, and acting applications. Wait, but the question is about Hugging Face, not LangChain.\n",
      "\n",
      "So, focusing back on Hugging Face. They provide models, which is key because without good models, NLP tasks would be harder. They also offer these models through an API, making it easier for developers to use without having to host the models themselves. Plus, the models can be run locally, which gives flexibility depending on the use case.\n",
      "\n",
      "Additionally, Hugging Face's contribution isn't just about the models themselves. They have a community-driven approach, which probably leads to more innovation and collaboration in the NLP space. They might also provide tools and libraries that make it easier to implement NLP solutions, like their Transformers library which is widely used.\n",
      "\n",
      "So putting it all together, Hugging Face contributes by offering open-source NLP models, providing accessible ways to use these models (both locally and via API), fostering a community for collaboration, and providing tools that simplify NLP tasks. This makes NLP more accessible and advances the field by enabling researchers and developers to build upon their resources.\n",
      "</think>\n",
      "\n",
      "Hugging Face contributes to NLP by providing open-source models for various tasks, including embeddings, which can be accessed locally or via an API. They foster innovation through community collaboration and offer tools like their Transformers library, simplifying NLP implementation and advancing the field.\n"
     ]
    }
   ],
   "source": [
    "question = \"How does Hugging Face contribute to NLP?\"\n",
    "ai_answer = langsmith_rag_with_wrap_groq(question)\n",
    "print(ai_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "acf55c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS vector database from faiss_index_hf with Hugging Face embeddings...\n",
      "FAISS vector database loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import groq\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils_hf_embeddings import get_hf_vector_db_retriever \n",
    "import os \n",
    "\n",
    "MODEL_PROVIDER = \"groq\"\n",
    "MODEL_NAME = \"deepseek-r1-distill-llama-70b\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "groq_client = groq.Groq()\n",
    "retriever = get_hf_vector_db_retriever() \n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_groq(messages)\n",
    "\n",
    "@traceable\n",
    "def call_groq(\n",
    "    messages: List[dict],\n",
    ") -> str:\n",
    "    return groq_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag_with_wrap_groq(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2c482a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I'm trying to figure out the primary purpose of LangChain. I remember seeing some context provided earlier about it. Let me go through that again. \n",
      "\n",
      "The context mentions that LangChain is a framework for developing applications powered by language models. It enables applications that are context-aware, reason, and can act. So, it's used for building apps with these advanced capabilities.\n",
      "\n",
      "Wait, the user's question is asking for the primary purpose, so I should focus on what it's mainly used for. From the context, it's clear that LangChain is a framework that helps create applications using language models, making them context-aware and capable of reasoning and acting.\n",
      "\n",
      "I don't see any other details about its other purposes, so the main purpose must be to provide the infrastructure for such applications. I should keep the answer concise and within three sentences.\n",
      "</think>\n",
      "\n",
      "The primary purpose of LangChain is to provide a framework for developing applications powered by language models, enabling them to be context-aware, reason, and act.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What is the primary purpose of LangChain?\" \n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e91f5878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS vector database from faiss_index_hf with Hugging Face embeddings...\n",
      "FAISS vector database loaded.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable, trace\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils_hf_embeddings import get_hf_vector_db_retriever \n",
    "import os \n",
    "\n",
    "MODEL_PROVIDER = \"groq\"\n",
    "MODEL_NAME = \"deepseek-r1-distill-llama-70b\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\" \n",
    "groq_client = Groq()\n",
    "retriever = get_hf_vector_db_retriever() \n",
    "\n",
    "@traceable\n",
    "def retrieve_documents(question: str):\n",
    "    documents = retriever.invoke(question)\n",
    "    return documents\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    response = call_groq(messages)\n",
    "    return response\n",
    "\n",
    "@traceable\n",
    "def call_groq(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b925715d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Can tracing be set up when the code is running in langsmith?',\n",
       " 'messages': [HumanMessage(content='Can tracing be set up when the code is running in langsmith?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"<think>\\nOkay, so the user is asking if tracing can be set up when the code is running in Langsmith. I don't have any specific information about Langsmith in the context provided. The context talks about LangChain, Hugging Face, and FAISS, but nothing about Langsmith. Since I don't have details on Langsmith's features or capabilities, I can't confirm whether tracing is possible there. I should let the user know that I don't have the answer based on the provided context.\\n</think>\\n\\nI don't know.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 111, 'prompt_tokens': 165, 'total_tokens': 276, 'completion_time': 0.47511181, 'prompt_time': 0.029352574, 'queue_time': 0.0420559, 'total_time': 0.504464384}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3ae82526-b3fe-4df1-a7e1-088dd574eacc-0', usage_metadata={'input_tokens': 165, 'output_tokens': 111, 'total_tokens': 276})],\n",
       " 'documents': [Document(id='2cc0d07f-b3ab-4d2f-90ce-bb2757e21a82', metadata={'source': 'state_of_the_union.txt'}, page_content='The quick brown fox jumps over the lazy dog. This is a sample document for testing Hugging Face embeddings and FAISS vector store functionality.\\n\\nLangChain is a framework for developing applications powered by language models. It enables applications that are context-aware, reason, and can act.\\n\\nHugging Face provides open-source models for various NLP tasks, including embeddings. These models can be run locally or via their inference API.')]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cell 3\n",
    "question = \"Can tracing be set up when the code is running in langsmith?\"\n",
    "simple_rag_graph.invoke({\"question\": question}, config={\"metadata\": {\"foo\": \"bar\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "405769e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS vector database from faiss_index_hf with Hugging Face embeddings...\n",
      "FAISS vector database loaded.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() \n",
    "\n",
    "import operator\n",
    "from langchain.schema import Document\n",
    "from langchain_core.messages import HumanMessage, AnyMessage, get_buffer_string\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from utils_hf_embeddings import get_hf_vector_db_retriever, RAG_PROMPT \n",
    "import os \n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\" \n",
    "\n",
    "retriever = get_hf_vector_db_retriever()\n",
    "llm = ChatGroq(model_name=\"deepseek-r1-distill-llama-70b\", temperature=0)\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    documents: List[Document]\n",
    "\n",
    "def retrieve_documents(state: GraphState):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(f\"{get_buffer_string(messages)} {question}\")\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def generate_response(state: GraphState):\n",
    "    question = state[\"question\"]\n",
    "    messages = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, conversation=messages, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"documents\": documents, \"messages\": [HumanMessage(question), generation]}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(GraphState)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"generate_response\")\n",
    "graph_builder.add_edge(\"generate_response\", END)\n",
    "\n",
    "simple_rag_graph = graph_builder.compile()\n",
    "display(Image(simple_rag_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e53c22c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAT496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
