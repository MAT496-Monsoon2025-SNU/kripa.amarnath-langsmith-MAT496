{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e428995e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking how to mark a trace as successful or failed in LangChain using LangSmith. I remember that the context provided talks about adding metadata and tags using the 'langsmith_extra' parameter. \n",
      "\n",
      "Hmm, the context doesn't mention anything specific about marking traces as successful or failed. It only explains how to add metadata and tags, like setting a 'metadata' key or adding tags to the dictionary. So, maybe the user is looking for a way to categorize their traces based on execution results.\n",
      "\n",
      "I think the best approach is to suggest using tags for this purpose. Since tags are used to categorize and filter traces, they could add a tag like 'success' or 'failed' in the 'tags' list within 'langsmith_extra'. That way, they can easily filter their traces in LangSmith based on these tags.\n",
      "\n",
      "But wait, the context doesn't explicitly state that this is the method for marking success or failure. So I should let the user know that while I don't have specific information on that, using tags is a viable workaround based on the provided context.\n",
      "</think>\n",
      "\n",
      "To mark a trace as successful or failed, you can use the 'tags' feature within the 'langsmith_extra' parameter. Add a tag like 'success' or 'failed' to the 'tags' list in the dictionary. This will help categorize and filter traces in LangSmith.\n",
      "\n",
      "Example: `{'langsmith_extra': {'tags': ['success']}}`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"How can I mark a trace as successful or failed?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ee8ae18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking if they can group traces by a custom field in LangSmith. Let me think about what I know from the context provided.\n",
      "\n",
      "The context talks about adding metadata and tags to LangChain traces using the 'langsmith_extra' parameter. It mentions that metadata can be attached with a dictionary, like having a 'metadata' key. Also, tags can be added as a list under the 'tags' key within the same parameter. These tags help in categorizing and filtering traces in LangSmith.\n",
      "\n",
      "So, if the user wants to group traces by a custom field, they have two options. They can either use the 'metadata' field to include a custom key-value pair or add specific tags. Both methods allow for categorization. The user would need to structure their 'langsmith_extra' parameter accordingly when setting up their traces. This way, they can later use LangSmith's features to group based on these additions.\n",
      "\n",
      "I don't see any other methods mentioned in the context, so I can't suggest anything else. It's possible that only these two approaches are supported right now.\n",
      "</think>\n",
      "\n",
      "Yes, you can group traces by custom fields using metadata or tags within the 'langsmith_extra' parameter. You can add a 'metadata' dictionary with a custom field like {'metadata': {'custom_field': 'value'}} or use 'tags' for categorization. Both methods help in organizing and filtering traces in LangSmith.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Can I group traces by a custom field?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(ai_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1bc3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store at ./sklearn_vectorstore_hf.\n",
      "Vector store created and persisted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langsmith import traceable\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils_hf_embeddings import get_vector_db_retriever_hf\n",
    "\n",
    "groq_client = Groq(api_key=\"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\")\n",
    "nest_asyncio.apply()\n",
    "\n",
    "retriever = get_vector_db_retriever_hf()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_groq(messages)\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_groq(\n",
    "    messages: List[dict], model: str = \"deepseek-r1-distill-llama-70b\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    return groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bbe257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import uuid\n",
    "thread_id = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92c028d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAT496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
