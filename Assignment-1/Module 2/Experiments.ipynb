{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b81c0dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'groq metadata added-85a525dd' at:\n",
      "https://smith.langchain.com/o/7078c9e6-ede7-4d80-b45b-119f20137777/datasets/83a50dcb-dc34-479f-a8bf-38efc64e61b8/compare?selectedSessions=59ed46c6-3d08-44d0-bdec-e189f73d3877\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8408eb36244290b14d021cf2be7abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 53b164c7-2360-407b-9124-d89e6a64c8e2: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run c1a8f0fc-3805-4b52-9a5a-517af4027466: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.wrapper</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I trace my LLM application?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so the user is asking how to tr...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can trace your LLM application using the L...</td>\n",
       "      <td>None</td>\n",
       "      <td>2.181237</td>\n",
       "      <td>f28c674b-0d68-4418-9d7e-4b23f3a1d724</td>\n",
       "      <td>53b164c7-2360-407b-9124-d89e6a64c8e2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith's primary purpose?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to figure out LangSmi...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith helps with developing, monitoring, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.635258</td>\n",
       "      <td>0ef09dbc-bbb6-44f7-a512-bc0a6824b045</td>\n",
       "      <td>c1a8f0fc-3805-4b52-9a5a-517af4027466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults groq metadata added-85a525dd>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"groq metadata added\", \n",
    "    metadata={  \n",
    "        \"model_name\": MODEL_NAME\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3197cae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'groq concurrency-4f019d03' at:\n",
      "https://smith.langchain.com/o/7078c9e6-ede7-4d80-b45b-119f20137777/datasets/83a50dcb-dc34-479f-a8bf-38efc64e61b8/compare?selectedSessions=54ce64c7-d2e3-4d9b-be23-ecaf86a612ea\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd0b63e723f413fb5593926dd121fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 4e718c19-3281-41ac-8314-bb31bc8f4d29: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 788d00b0-6c34-421a-8110-a3d26ffcfdd6: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.wrapper</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LangSmith's primary purpose?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, I need to figure out LangSmith'...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith helps with developing, monitoring, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.481295</td>\n",
       "      <td>0ef09dbc-bbb6-44f7-a512-bc0a6824b045</td>\n",
       "      <td>4e718c19-3281-41ac-8314-bb31bc8f4d29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I trace my LLM application?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to figure out how to ...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can trace your LLM application using the L...</td>\n",
       "      <td>None</td>\n",
       "      <td>5.153045</td>\n",
       "      <td>f28c674b-0d68-4418-9d7e-4b23f3a1d724</td>\n",
       "      <td>788d00b0-6c34-421a-8110-a3d26ffcfdd6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults groq concurrency-4f019d03>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"groq concurrency\", \n",
    "    max_concurrency=3,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21460399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'groq two repetitions-f436bae2' at:\n",
      "https://smith.langchain.com/o/7078c9e6-ede7-4d80-b45b-119f20137777/datasets/83a50dcb-dc34-479f-a8bf-38efc64e61b8/compare?selectedSessions=9199e683-9fa5-4fe4-b2e2-5a3f4d7c75d5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8939fe73240e4b2492bcef135f376885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run e4a14169-4022-45cf-8ffe-29d315090b54: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 7582ea63-ae51-461f-bd05-d977f3b0f814: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 042d4cb3-a47d-41f3-8dd4-82aa69b92ae6: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 1fe04e91-f2df-49ba-8720-ac5b4816b87e: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.wrapper</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I trace my LLM application?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so the user is asking how to tr...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can trace your LLM application using the L...</td>\n",
       "      <td>None</td>\n",
       "      <td>2.481663</td>\n",
       "      <td>f28c674b-0d68-4418-9d7e-4b23f3a1d724</td>\n",
       "      <td>e4a14169-4022-45cf-8ffe-29d315090b54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith's primary purpose?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to figure out the pri...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith helps with developing, monitoring, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.256953</td>\n",
       "      <td>0ef09dbc-bbb6-44f7-a512-bc0a6824b045</td>\n",
       "      <td>7582ea63-ae51-461f-bd05-d977f3b0f814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I trace my LLM application?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so the user is asking how to tr...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can trace your LLM application using the L...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.507273</td>\n",
       "      <td>f28c674b-0d68-4418-9d7e-4b23f3a1d724</td>\n",
       "      <td>042d4cb3-a47d-41f3-8dd4-82aa69b92ae6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is LangSmith's primary purpose?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to figure out what La...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith helps with developing, monitoring, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.618343</td>\n",
       "      <td>0ef09dbc-bbb6-44f7-a512-bc0a6824b045</td>\n",
       "      <td>1fe04e91-f2df-49ba-8720-ac5b4816b87e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults groq two repetitions-f436bae2>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"groq two repetitions\", \n",
    "    num_repetitions=2  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a41330e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'two specific groq example ids-4e6faac9' at:\n",
      "https://smith.langchain.com/o/7078c9e6-ede7-4d80-b45b-119f20137777/datasets/83a50dcb-dc34-479f-a8bf-38efc64e61b8/compare?selectedSessions=b7367bae-181e-4f70-b127-83503ec601e7\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617ae136b52f430fb128660537a7cd12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run e53335d1-106d-4f11-bc06-2e61435e5999: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.wrapper</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LangSmith's primary purpose?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I'm trying to figure out wha...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith helps with developing, monitoring, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.983563</td>\n",
       "      <td>0ef09dbc-bbb6-44f7-a512-bc0a6824b045</td>\n",
       "      <td>e53335d1-106d-4f11-bc06-2e61435e5999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults two specific groq example ids-4e6faac9>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=client.list_examples(\n",
    "        dataset_name=dataset_name,\n",
    "        example_ids=[   \n",
    "             \"0ef09dbc-bbb6-44f7-a512-bc0a6824b045\",\n",
    "            \"3b31c869-d0f3-4a7a-b1ec-c85b45bb4418\"\n",
    "        ]\n",
    "    ),\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"two specific groq example ids\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2f8b8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Critical Groq Scenarios split-43b6f27e' at:\n",
      "https://smith.langchain.com/o/7078c9e6-ede7-4d80-b45b-119f20137777/datasets/83a50dcb-dc34-479f-a8bf-38efc64e61b8/compare?selectedSessions=74618f1d-9558-4d8c-aef1-2a00a1290158\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbddc9b64a34cca9278c0b8d557f80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 57514fd3-748a-426e-b498-69180c5b73da: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 4bc21e6d-f26c-4026-a5dc-f2f44e6c0153: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.wrapper</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I trace my LLM application?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so the user is asking how to tr...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can trace your LLM application using the L...</td>\n",
       "      <td>None</td>\n",
       "      <td>7.730916</td>\n",
       "      <td>f28c674b-0d68-4418-9d7e-4b23f3a1d724</td>\n",
       "      <td>57514fd3-748a-426e-b498-69180c5b73da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith's primary purpose?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to figure out LangSmi...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith helps with developing, monitoring, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>2.926612</td>\n",
       "      <td>0ef09dbc-bbb6-44f7-a512-bc0a6824b045</td>\n",
       "      <td>4bc21e6d-f26c-4026-a5dc-f2f44e6c0153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults Critical Groq Scenarios split-43b6f27e>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=client.list_examples(dataset_name=dataset_name),  \n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"Critical Groq Scenarios split\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd511ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'initial groq dataset version-11212d46' at:\n",
      "https://smith.langchain.com/o/7078c9e6-ede7-4d80-b45b-119f20137777/datasets/83a50dcb-dc34-479f-a8bf-38efc64e61b8/compare?selectedSessions=8d7a7483-fc2b-42ad-8808-ccdbf0b571a7\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee988a733ef45b3b3c89f37ead2c97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 30766e46-5c06-408e-a7a0-96738d5534f5: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run d310d506-af58-486c-9292-fb239de08df1: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.wrapper</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I trace my LLM application?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so the user is asking how to tr...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can trace your LLM application using the L...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.932194</td>\n",
       "      <td>f28c674b-0d68-4418-9d7e-4b23f3a1d724</td>\n",
       "      <td>30766e46-5c06-408e-a7a0-96738d5534f5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith's primary purpose?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I'm trying to figure out wha...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith helps with developing, monitoring, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.321284</td>\n",
       "      <td>0ef09dbc-bbb6-44f7-a512-bc0a6824b045</td>\n",
       "      <td>d310d506-af58-486c-9292-fb239de08df1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults initial groq dataset version-11212d46>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=client.list_examples(dataset_name=dataset_name),  \n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"initial groq dataset version\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56d5033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'groq-deepseek-alternate-e35b9512' at:\n",
      "https://smith.langchain.com/o/7078c9e6-ede7-4d80-b45b-119f20137777/datasets/83a50dcb-dc34-479f-a8bf-38efc64e61b8/compare?selectedSessions=52228763-dba1-4bf6-a321-dfdc1c7d9ad3\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624f94b143574c8bbf09b5afa7e70e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run f0cbc94c-4d99-4bdd-8104-ca375d1c6959: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 4b57ce45-c65d-422c-a4e4-06396ca31d56: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.wrapper</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I trace my LLM application?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so the user is asking how to tr...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can trace your LLM application using the L...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.911283</td>\n",
       "      <td>f28c674b-0d68-4418-9d7e-4b23f3a1d724</td>\n",
       "      <td>f0cbc94c-4d99-4bdd-8104-ca375d1c6959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith's primary purpose?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so the user is asking about Lan...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith helps with developing, monitoring, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.062061</td>\n",
       "      <td>0ef09dbc-bbb6-44f7-a512-bc0a6824b045</td>\n",
       "      <td>4b57ce45-c65d-422c-a4e4-06396ca31d56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults groq-deepseek-alternate-e35b9512>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate, Client\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def target_function(inputs: dict):\n",
    "    return langsmith_rag(inputs[\"question\"])\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"groq-deepseek-alternate\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81647162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'RAG Application Golden Dataset - Groq Test' already exists.\n",
      "View the evaluation results for experiment: 'groq-deepseek-65aa8b76' at:\n",
      "https://smith.langchain.com/o/7078c9e6-ede7-4d80-b45b-119f20137777/datasets/83a50dcb-dc34-479f-a8bf-38efc64e61b8/compare?selectedSessions=8d3fe114-b101-43c8-bfc1-0f9c072419e8\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5c1f816ab741c3956b1e8623ff8165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 3d88428f-513c-44cf-b922-139af8401a9d: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run f89721e5-a7be-4cdf-bc2d-0b7b415c4133: KeyError('output')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "        func, *args, **kwargs\n",
      "    )\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\MAT496\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9276\\2706090917.py\", line 34, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
      "                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'output'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.wrapper</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I trace my LLM application?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so the user is asking how to tr...</td>\n",
       "      <td>None</td>\n",
       "      <td>You can trace your LLM application using the L...</td>\n",
       "      <td>None</td>\n",
       "      <td>2.045204</td>\n",
       "      <td>f28c674b-0d68-4418-9d7e-4b23f3a1d724</td>\n",
       "      <td>3d88428f-513c-44cf-b922-139af8401a9d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is LangSmith's primary purpose?</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to figure out what La...</td>\n",
       "      <td>None</td>\n",
       "      <td>LangSmith helps with developing, monitoring, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.921373</td>\n",
       "      <td>0ef09dbc-bbb6-44f7-a512-bc0a6824b045</td>\n",
       "      <td>f89721e5-a7be-4cdf-bc2d-0b7b415c4133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults groq-deepseek-65aa8b76>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate, Client\n",
    "from langsmith.schemas import Example \n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG Application Golden Dataset - Groq Test\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(f\"Dataset '{dataset_name}' already exists.\")\n",
    "except Exception: \n",
    "    print(f\"Dataset '{dataset_name}' not found. Creating it now...\")\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Golden dataset for RAG application evaluation with Groq.\"\n",
    "    )\n",
    "    print(f\"Dataset '{dataset_name}' created.\")\n",
    "    \n",
    "    client.create_example(\n",
    "        dataset_id=dataset.id,\n",
    "        inputs={\"question\": \"What is LangSmith's primary purpose?\"},\n",
    "        outputs={\"answer\": \"LangSmith helps with developing, monitoring, and evaluating language model applications.\"}\n",
    "    )\n",
    "    client.create_example(\n",
    "        dataset_id=dataset.id,\n",
    "        inputs={\"question\": \"How can I trace my LLM application?\"},\n",
    "        outputs={\"answer\": \"You can trace your LLM application using the LangSmith client to log runs and visualize them in the LangSmith UI.\"}\n",
    "    )\n",
    "    print(\"Added sample examples to the dataset.\")\n",
    "\n",
    "\n",
    "def is_concise_enough(reference_outputs: dict, outputs: dict) -> dict:\n",
    "   \n",
    "    score = len(outputs[\"output\"]) < 2 * len(reference_outputs[\"output\"])\n",
    "    return {\"key\": \"is_concise\", \"score\": int(score)}\n",
    "\n",
    "def target_function(inputs: dict):\n",
    "    return langsmith_rag(inputs[\"question\"])\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"groq-deepseek\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0607466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langsmith import traceable\n",
    "from groq import Groq \n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "MODEL_NAME = \"deepseek-r1-distill-llama-70b\" \n",
    "MODEL_PROVIDER = \"groq\" \n",
    "APP_VERSION = 1.1\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "groq_client = Groq(\n",
    "    api_key=\"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\"\n",
    ")\n",
    "huggingface_embeddings_client = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union_hf_groq.parquet\")\n",
    "    embd = huggingface_embeddings_client \n",
    "\n",
    "    \n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_groq` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_groq(messages) \n",
    "\n",
    "\"\"\"\n",
    "call_groq\n",
    "- Returns the chat completion output from Groq\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_groq(messages: List[dict]) -> str:\n",
    "    return groq_client.chat.completions.create( \n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "272e0c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAT496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
