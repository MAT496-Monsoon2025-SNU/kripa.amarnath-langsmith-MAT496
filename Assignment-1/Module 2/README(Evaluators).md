Evaluators are the core logic responsible for determining if your application's output is good or bad for a given test case. 
They produce metrics that quantify performance.

-Inputs to Evaluation: An evaluator takes three key pieces of information for a single test run:
 1.The original Input from the dataset example.
 2.The Reference Output from the dataset example.
 3.The Run Output (the actual response generated by the application).

-Metric Calculation: Evaluators use these three pieces to calculate metrics. Examples include boolean scores (Correctness: True/False) 
                     or continuous scores (Toxicity: 0.1-1.0).

-Types:
1.LLM-as-a-Judge: An LLM is used to score the output based on a set of criteria and prompts. This is powerful but more costly than
                  non-LLM methods.
2.Custom Code Evaluators: Logic written by the developer.
3.Heuristic/Pre-built Evaluators: Standard metrics like latency, error counts, or basic checks (e.g., JSON validity).
