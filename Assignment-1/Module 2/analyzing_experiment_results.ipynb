{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5692edd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Listing Recent Experiments ---\n",
      "Error reading project 'RAG Application Golden Dataset - Groq Test': Project RAG Application Golden Dataset - Groq Test not found\n",
      "Please ensure the dataset name is correct and you have access.\n",
      "\n",
      "--- Comparing Multiple Experiments ---\n",
      "Error reading project 'RAG Application Golden Dataset - Groq Test': Project RAG Application Golden Dataset - Groq Test not found\n",
      "Please ensure the dataset name is correct and you have access.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.schemas import Run\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def analyze_langsmith_experiments(dataset_name: str, target_experiment_prefix: str):\n",
    "    \"\"\"\n",
    "    Analyzes LangSmith experiments for a given dataset and target experiment prefix.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): The name of the LangSmith dataset.\n",
    "        target_experiment_prefix (str): The prefix of the experiment to analyze in detail.\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "\n",
    "    # Listing Experiments \n",
    "    print(\"--- Listing Recent Experiments ---\")\n",
    "    try:\n",
    "        experiments_project_name = client.read_project(project_name=dataset_name).name\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading project '{dataset_name}': {e}\")\n",
    "        print(\"Please ensure the dataset name is correct and you have access.\")\n",
    "        return\n",
    "\n",
    "    experiments = client.list_runs(\n",
    "        project_name=experiments_project_name,\n",
    "        run_type=\"experiment\"\n",
    "    )\n",
    "\n",
    "    experiments_list = list(experiments)\n",
    "    print(f\"Found {len(experiments_list)} experiments in project '{experiments_project_name}':\")\n",
    "    for i, exp in enumerate(experiments_list[:5]):\n",
    "        print(f\"  {i+1}. Name: {exp.name}, ID: {exp.id}, Created: {exp.created_at}\")\n",
    "        if exp.metadata:\n",
    "            print(f\"     Metadata: {exp.metadata}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Fetching Runs for a Specific Experiment \n",
    "    target_experiment_run = None\n",
    "    for exp in experiments_list:\n",
    "        if exp.name.startswith(target_experiment_prefix):\n",
    "            target_experiment_run = exp\n",
    "            break\n",
    "\n",
    "    if target_experiment_run:\n",
    "        print(f\"\\n--- Analyzing Experiment: '{target_experiment_run.name}' (ID: {target_experiment_run.id}) ---\")\n",
    "\n",
    "        experiment_runs = client.list_runs(\n",
    "            project_name=experiments_project_name,\n",
    "            parent_run_id=target_experiment_run.id,\n",
    "        )\n",
    "\n",
    "        all_run_data = []\n",
    "        print(f\"Fetching evaluation results for {target_experiment_run.name}...\")\n",
    "        for run in experiment_runs:\n",
    "            scores = run.feedback\n",
    "            inputs = run.inputs\n",
    "            outputs = run.outputs\n",
    "\n",
    "            run_info = {\n",
    "                \"run_id\": run.id,\n",
    "                \"example_id\": run.reference_example_id,\n",
    "                \"question\": inputs.get(\"question\") if inputs else \"N/A\",\n",
    "                \"model_output\": outputs.get(\"output\") if outputs else \"N/A\",\n",
    "                \"status\": run.status,\n",
    "                \"start_time\": run.start_time,\n",
    "                \"end_time\": run.end_time,\n",
    "                \"duration_ms\": (run.end_time - run.start_time).total_seconds() * 1000 if run.start_time and run.end_time else None\n",
    "            }\n",
    "\n",
    "            for score in scores:\n",
    "                run_info[f\"score_{score.key}\"] = score.score\n",
    "                if score.comment:\n",
    "                    run_info[f\"comment_{score.key}\"] = score.comment\n",
    "            all_run_data.append(run_info)\n",
    "\n",
    "        if not all_run_data:\n",
    "            print(f\"No individual runs found for experiment '{target_experiment_run.name}'. \"\n",
    "                  \"Please ensure the experiment ran successfully and generated evaluation runs.\")\n",
    "        else:\n",
    "            df = pd.DataFrame(all_run_data)\n",
    "            print(\"\\n--- Individual Run Review (First 5 Rows) ---\")\n",
    "            print(df.head())\n",
    "\n",
    "            print(\"\\n--- Summary Metrics ---\")\n",
    "            conciseness_scores_col = f\"score_is_concise\"\n",
    "            if conciseness_scores_col in df.columns:\n",
    "                conciseness_scores = df[conciseness_scores_col].mean()\n",
    "                print(f\"Average Conciseness Score: {conciseness_scores:.2f}\")\n",
    "            else:\n",
    "                print(f\"'{conciseness_scores_col}' not found in scores. Skipping conciseness analysis.\")\n",
    "\n",
    "            avg_duration = df[\"duration_ms\"].mean()\n",
    "            print(f\"Average Run Duration: {avg_duration:.2f} ms\")\n",
    "\n",
    "            if conciseness_scores_col in df.columns:\n",
    "                print(\"\\n--- Visualizing Conciseness Scores ---\")\n",
    "                plt.figure(figsize=(8, 5))\n",
    "                sns.countplot(x=conciseness_scores_col, data=df)\n",
    "                plt.title(f\"Conciseness Scores for Experiment: {target_experiment_run.name}\")\n",
    "                plt.xlabel(\"Is Concise (0=No, 1=Yes)\")\n",
    "                plt.ylabel(\"Number of Runs\")\n",
    "                plt.xticks([0, 1])\n",
    "                plt.show() \n",
    "            \n",
    "            print(\"\\n--- Visualizing Run Duration Distribution ---\")\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(df[\"duration_ms\"].dropna(), kde=True)\n",
    "            plt.title(f\"Distribution of Run Durations for Experiment: {target_experiment_run.name}\")\n",
    "            plt.xlabel(\"Duration (ms)\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show() \n",
    "\n",
    "            print(\"\\n--- Debugging and Error Analysis (Example) ---\")\n",
    "            if conciseness_scores_col in df.columns:\n",
    "                not_concise_runs = df[df[conciseness_scores_col] == 0]\n",
    "                if not not_concise_runs.empty:\n",
    "                    print(f\"\\nExamples that were NOT concise (first 2):\")\n",
    "                    for i, row in not_concise_runs.head(2).iterrows():\n",
    "                        print(f\"  Run ID: {row['run_id']}\")\n",
    "                        print(f\"  Example ID: {row['example_id']}\")\n",
    "                        print(f\"  Question: {row['question']}\")\n",
    "                        print(f\"  Model Output: {row['model_output']}\")\n",
    "                        comment_col = f\"comment_is_concise\"\n",
    "                        if comment_col in row and pd.notna(row[comment_col]):\n",
    "                            print(f\"  Conciseness Comment: {row[comment_col]}\")\n",
    "                        print(\"-\" * 20)\n",
    "                else:\n",
    "                    print(\"\\nAll runs were concise, no non-concise examples found for debugging.\")\n",
    "            else:\n",
    "                print(f\"Cannot perform conciseness debugging as '{conciseness_scores_col}' was not found.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Experiment with prefix '{target_experiment_prefix}' not found. \"\n",
    "              \"Please check your experiment prefixes or ensure experiments have run.\")\n",
    "\n",
    "def compare_langsmith_experiments(dataset_name: str, experiment_prefixes_to_compare: List[str]):\n",
    "    \"\"\"\n",
    "    Compares multiple LangSmith experiments based on conciseness and duration.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): The name of the LangSmith dataset.\n",
    "        experiment_prefixes_to_compare (List[str]): A list of experiment prefixes to compare.\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "    print(\"\\n--- Comparing Multiple Experiments ---\")\n",
    "\n",
    "    try:\n",
    "        experiments_project_name = client.read_project(project_name=dataset_name).name\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading project '{dataset_name}': {e}\")\n",
    "        print(\"Please ensure the dataset name is correct and you have access.\")\n",
    "        return\n",
    "\n",
    "    experiments = client.list_runs(\n",
    "        project_name=experiments_project_name,\n",
    "        run_type=\"experiment\"\n",
    "    )\n",
    "    experiments_list = list(experiments)\n",
    "\n",
    "    comparison_data = []\n",
    "\n",
    "    for prefix in experiment_prefixes_to_compare:\n",
    "        exp_run = None\n",
    "        for exp in experiments_list:\n",
    "            if exp.name.startswith(prefix):\n",
    "                exp_run = exp\n",
    "                break\n",
    "\n",
    "        if exp_run:\n",
    "            print(f\"Collecting data for experiment: {exp_run.name}\")\n",
    "            child_runs = client.list_runs(\n",
    "                project_name=experiments_project_name,\n",
    "                parent_run_id=exp_run.id\n",
    "            )\n",
    "            current_exp_scores = []\n",
    "            current_exp_durations = []\n",
    "            for run in child_runs:\n",
    "                for score in run.feedback:\n",
    "                    if score.key == \"is_concise\":\n",
    "                        current_exp_scores.append(score.score)\n",
    "                if run.start_time and run.end_time:\n",
    "                    current_exp_durations.append((run.end_time - run.start_time).total_seconds() * 1000)\n",
    "\n",
    "            if current_exp_scores:\n",
    "                comparison_data.append({\n",
    "                    \"Experiment\": exp_run.name,\n",
    "                    \"Average Conciseness\": sum(current_exp_scores) / len(current_exp_scores),\n",
    "                    \"Average Duration (ms)\": sum(current_exp_durations) / len(current_exp_durations) if current_exp_durations else None,\n",
    "                    \"Number of Runs\": len(current_exp_scores)\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Experiment with prefix '{prefix}' not found.\")\n",
    "\n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(\"\\n--- Experiment Comparison Summary ---\")\n",
    "        print(comparison_df.set_index(\"Experiment\"))\n",
    "\n",
    "        if len(comparison_data) > 1:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.barplot(x=\"Experiment\", y=\"Average Conciseness\", data=comparison_df)\n",
    "            plt.title(\"Average Conciseness Score by Experiment\")\n",
    "            plt.ylabel(\"Score (0-1)\")\n",
    "            plt.ylim(0, 1)\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sns.barplot(x=\"Experiment\", y=\"Average Duration (ms)\", data=comparison_df)\n",
    "            plt.title(\"Average Run Duration by Experiment\")\n",
    "            plt.ylabel(\"Duration (ms)\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show() \n",
    "    else:\n",
    "        print(\"No data collected for comparison.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset_name = \"RAG Application Golden Dataset - Groq Test\" \n",
    "\n",
    "    target_experiment_prefix = \"groq-deepseek\" \n",
    "    analyze_langsmith_experiments(dataset_name, target_experiment_prefix)\n",
    "\n",
    "   \n",
    "    experiment_prefixes_to_compare = [\"groq-deepseek\", \"groq-deepseek-alternate\"] \n",
    "    compare_langsmith_experiments(dataset_name, experiment_prefixes_to_compare)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAT496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
