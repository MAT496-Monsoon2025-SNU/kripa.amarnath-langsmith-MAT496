{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2a96450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What are the key features of LangSmith for LLM application development?\n",
      "Response 1: <think>\n",
      "Okay, I need to figure out the key features of LangSmith for LLM application development. Let me look through the provided context to find relevant information.\n",
      "\n",
      "First, the context mentions that LangSmith is a platform for building production-grade LLM applications. It emphasizes observability and evaluation, which suggests it has tools for monitoring and assessing applications.\n",
      "\n",
      "I see sections about tracing an application, which gives visibility into each step for debugging. That's probably one feature. Then there's evaluating the application to measure quality over time, which is another key aspect. Additionally, there's something about testing prompts with version control and collaboration features, so that's a third feature.\n",
      "\n",
      "The context also notes that LangSmith is framework-agnostic, meaning it can be used with or without LangChain's frameworks. This flexibility is another important feature.\n",
      "\n",
      "Putting it all together, the key features are observability, evaluation, prompt testing, framework agnosticism, and collaboration tools.\n",
      "</think>\n",
      "\n",
      "LangSmith offers several key features for LLM application development, including observability to monitor application steps, evaluation to measure quality over time, prompt testing with version control, framework-agnostic support, and collaboration tools for team workflows. These features help developers build and ship reliable AI applications confidently.\n",
      "\n",
      "Question 2: Can LangSmith help with evaluating the performance of an agent over time?\n",
      "Response 2: <think>\n",
      "Okay, I'm trying to figure out if LangSmith can help evaluate an agent's performance over time. Let me go through the context provided.\n",
      "\n",
      "First, I see that LangSmith is a platform for LLM observability and evaluation. That suggests it's designed to monitor and assess how agents perform. There's a section about running evaluations and different types of evaluations, which implies it can handle various assessment methods.\n",
      "\n",
      "Looking at the tutorials, there's one on evaluating a complex agent. That makes me think LangSmith can handle detailed assessments for sophisticated systems. Another tutorial talks about backtesting a new version of an agent, which involves using historical data to compare performance before and after changes. This directly relates to tracking performance over time.\n",
      "\n",
      "Additionally, there's a section on analyzing experiment results and comparing them. This means LangSmith can help in understanding how an agent's performance changes over different experiments or versions. They also mention fetching performance metrics, which would be essential for tracking changes over time.\n",
      "\n",
      "The ability to audit evaluator scores and correct them manually adds another layer of accuracy, ensuring that the evaluations are reliable when assessing long-term performance.\n",
      "\n",
      "Putting it all together, LangSmith provides tools for setting up evaluations, backtesting with historical data, analyzing results over time, and ensuring accurate scoring. So yes, LangSmith can definitely help with evaluating an agent's performance over time.\n",
      "</think>\n",
      "\n",
      "Yes, LangSmith can help evaluate an agent's performance over time by running backtests using historical data, analyzing experiment results, and comparing performance metrics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question1 = \"What are the key features of LangSmith for LLM application development?\"\n",
    "response1 = langsmith_rag_hf(question1)\n",
    "print(f\"Question 1: {question1}\\nResponse 1: {response1}\\n\")\n",
    "\n",
    "question2 = \"Can LangSmith help with evaluating the performance of an agent over time?\"\n",
    "response2 = langsmith_rag_hf(question2)\n",
    "print(f\"Question 2: {question2}\\nResponse 2: {response2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9288822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from app_huggingface import langsmith_rag_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "742b47d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'LangSmith RAG Questions - HuggingFace Embeddings' already exists. Using existing dataset.\n",
      "Examples uploaded to LangSmith dataset: 'LangSmith RAG Questions - HuggingFace Embeddings' (ID: 5723f016-08d2-4585-8464-2537cbd8c2a3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langsmith import Client\n",
    "from langsmith import utils as ls_utils \n",
    "\n",
    "example_inputs = [\n",
    "(\"How do I set up tracing to LangSmith if I'm using LangChain?\", \"To set up tracing to LangSmith while using LangChain, you need to set the environment variable `LANGSMITH_TRACING` to 'true'. Additionally, you must set the `LANGSMITH_API_KEY` environment variable to your API key. By default, traces will be logged to a project named \\\"default.\\\"\"),\n",
    "(\"How can I trace with the @traceable decorator?\", \"To trace with the @traceable decorator in Python, simply decorate any function you want to log traces for by adding `@traceable` above the function definition. Ensure that the LANGSMITH_TRACING environment variable is set to 'true' to enable tracing, and also set the LANGSMITH_API_KEY environment variable with your API key. By default, traces will be logged to a project named \\\"default,\\\" but you can configure it to log to a different project if needed.\"),\n",
    "(\"How do I pass metadata in with @traceable?\", \"You can pass metadata with the @traceable decorator by specifying arbitrary key-value pairs as arguments. This allows you to associate additional information, such as the execution environment or user details, with your traces. For more detailed instructions, refer to the LangSmith documentation on adding metadata and tags.\"),\n",
    "(\"What is LangSmith used for in three sentences?\", \"LangSmith is a platform designed for the development, monitoring, and testing of LLM applications. It enables users to collect and analyze unstructured data, debug issues, and create datasets for testing and evaluation. The tool supports various workflows throughout the application development lifecycle, enhancing the overall performance and reliability of LLM applications.\"),\n",
    "(\"What testing capabilities does LangSmith have?\", \"LangSmith offers capabilities for creating datasets of inputs and reference outputs to run tests on LLM applications, supporting a test-driven approach. It allows for bulk uploads of test cases, on-the-fly creation, and exporting from application traces. Additionally, LangSmith facilitates custom evaluations to score test results, enhancing the testing process.\"),\n",
    "(\"Does LangSmith support online evaluation?\", \"Yes, LangSmith supports online evaluation as a feature. It allows you to configure a sample of runs from production to be evaluated, providing feedback on those runs. You can use either custom code or an LLM as a judge for the evaluations.\"),\n",
    "(\"Does LangSmith support offline evaluation?\", \"Yes, LangSmith supports offline evaluation through its evaluation how-to guides and features for managing datasets. Users can manage datasets for offline evaluations and run various types of evaluations, including unit testing and auto-evaluation. This allows for comprehensive testing and improvement of LLM applications.\"),\n",
    "(\"Can LangSmith be used for finetuning and model training?\", \"Yes, LangSmith can be used for fine-tuning and model training. It allows you to capture run traces from your deployment, query and filter this data, and convert it into a format suitable for fine-tuning models. Additionally, you can create training datasets to keep track of the data used for model training.\"),\n",
    "(\"Can LangSmith be used to evaluate agents?\", \"Yes, LangSmith can be used to evaluate agents. It provides various evaluation strategies, including assessing the agent's final response, evaluating individual steps, and analyzing the trajectory of tool calls. These methods help ensure the effectiveness of LLM applications.\"),\n",
    "(\"How do I create user feedback with the LangSmith sdk?\", \"To create user feedback with the LangSmith SDK, you first need to run your application and obtain the `run_id`. Then, you can use the `create_feedback` method, providing the `run_id`, a feedback key, a score, and an optional comment. For example, in Python, it would look like this: `client.create_feedback(run_id, key=\\\"feedback-key\\\", score=1.0, comment=\\\"comment\\\")`.\"),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"LangSmith RAG Questions - HuggingFace Embeddings\"\n",
    "\n",
    "\n",
    "try:\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(f\"Dataset '{dataset_name}' already exists. Using existing dataset.\")\n",
    "except ls_utils.LangSmithNotFoundError:\n",
    "    \n",
    "    print(f\"Dataset '{dataset_name}' not found. Creating a new dataset.\")\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Questions and answers about LangSmith for RAG evaluation (HuggingFace Embeddings)\"\n",
    "    )\n",
    "\n",
    "\n",
    "inputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]\n",
    "outputs = [{\"output\": output_answer} for _, output_answer in example_inputs]\n",
    "\n",
    "client.create_examples(\n",
    "  inputs=inputs,\n",
    "  outputs=outputs,\n",
    "  dataset_id=dataset.id,\n",
    ")\n",
    "\n",
    "print(f\"Examples uploaded to LangSmith dataset: '{dataset_name}' (ID: {dataset.id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deebfe02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAT496",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
